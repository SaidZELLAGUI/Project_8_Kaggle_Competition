{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cb85ae",
   "metadata": {},
   "source": [
    "# <font color=\"#1d479b\">**Projet 8: I'm something of a painter my self** </font>\n",
    "\n",
    "La vision par ordinateur a énormément progressé ces dernières années et les GANs sont désormais capables de mimer des objets de manière très convaincante.\n",
    "\n",
    "Dans ce challenge kaggle, le but est de générer des peinture style Monet à partir des photogaphies réelles prises par une caméra.\n",
    "\n",
    "Dataset\n",
    "\n",
    "    Monet_jpg : 300 peintures de Monet format 256x256 au format jpeg\n",
    "    monet_tfrec : 300 peintures de Monet format 256x256 au format tfrecord\n",
    "    photo_jpg : 7028 photos de taille 256x256 au format jpeg\n",
    "    photo_tfrec : 7028 photos de taille 256x256 au format tfrecord\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2667e",
   "metadata": {},
   "source": [
    "## <font color=\"#1d479b\" id=\"section_1\">**1. Importation des librairies**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fa2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ce107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    # main parameters\n",
    "\n",
    "    # Root directory for dataset\n",
    "    dataroot_A = \"train_photo/photo_jpg\"\n",
    "    dataroot_B = \"train_monet/monet_jpg\"\n",
    "    \n",
    "    dataroot_test_A = \"test_photo\"\n",
    "\n",
    "    # Number of workers for dataloader\n",
    "    workers = 4\n",
    "\n",
    "    # Batch size during training\n",
    "    batch_size = 4\n",
    "\n",
    "    # Spatial size of training images. All images will be resized to this\n",
    "    #   size using a transformer.\n",
    "    image_size = 256\n",
    "\n",
    "    # Number of channels in the group A\n",
    "    input_nc = 3\n",
    "    \n",
    "    # Number of channels in the group B\n",
    "    output_nc = 3\n",
    "\n",
    "    # number of filters in the last conv layer of generator\n",
    "    ngf = 32\n",
    "\n",
    "    # number of filters in the first conv layer of discriminator\n",
    "    ndf = 16\n",
    "\n",
    "    # Number of training epochs\n",
    "    n_epochs = 20\n",
    "\n",
    "    # Learning rate for optimizers\n",
    "    lr = 0.0002\n",
    "    \n",
    "    # identity_loss coeff.\n",
    "    lambda_identity = .5\n",
    "    lambda_A = 10.0\n",
    "    lambda_B = 10.0\n",
    "\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "\n",
    "    # Number of GPUs available. Use 0 for CPU mode.\n",
    "    ngpu = 1\n",
    "    \n",
    "    gan_mode = 'lsgan' # 'vanilla', 'wgangp'\n",
    "    \n",
    "    pool_size = 16\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    print_freq = 1\n",
    "    save_freq = 5\n",
    "    save_dir = \"/content/drive/MyDrive/data/\"\n",
    "opt = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c43383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to empty up the memory in GPU\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91184376",
   "metadata": {},
   "source": [
    "## <font color=\"#1d479b\" id=\"section_2\">**2. Chargement des données**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset to return pairs of images from both 2 styles\n",
    "class UnpairedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, opt):\n",
    "        self.path_A = opt.dataroot_A\n",
    "        self.path_B = opt.dataroot_B\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(opt.image_size),\n",
    "            transforms.CenterCrop(opt.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        \n",
    "        self.datasetA = dset.ImageFolder(root = self.path_A, transform = transform)\n",
    "        self.datasetB = dset.ImageFolder(root = self.path_B, transform = transform)\n",
    "        \n",
    "        self.size_A = len(self.datasetA)\n",
    "        self.size_B = len(self.datasetB)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(self.size_A, self.size_B)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        A_tensor = self.datasetA[index % self.size_A][0]\n",
    "        \n",
    "        index_B = random.randint(0, self.size_B - 1)\n",
    "        B_tensor = self.datasetB[index_B][0]\n",
    "        \n",
    "        return {'A': A_tensor, 'B': B_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpaired_dataset = UnpairedDataset(opt)\n",
    "dataloader = torch.utils.data.DataLoader(unpaired_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a batch of data\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=128)\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"(A): actual photos\")\n",
    "\n",
    "grid_A = vutils.make_grid(real_batch['A'], padding=2, normalize=True, nrow=4).cpu()\n",
    "plt.imshow(np.transpose(grid_A,(1,2,0)))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# plt.figure(figsize=(16,4), dpi=128)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"(B): Monet paintings\")\n",
    "grid_B = vutils.make_grid(real_batch['B'], padding=2, normalize=True, nrow=4).cpu()\n",
    "plt.imshow(np.transpose(grid_B,(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dset.ImageFolder(\n",
    "    root = opt.dataroot_test_A,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(opt.image_size),\n",
    "        transforms.CenterCrop(opt.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]))\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed batch used to visualize the evolution of the network over training\n",
    "fixed_test_batch = next(iter(dataloader_test))[0].to(opt.device)\n",
    "\n",
    "plt.figure(figsize=(4,4), dpi=128)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"(A): fixed_batch\")\n",
    "grid_fixed = vutils.make_grid(fixed_test_batch, padding=2, normalize=True, nrow=2).cpu()\n",
    "plt.imshow(np.transpose(grid_fixed,(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab42d9",
   "metadata": {},
   "source": [
    "## <font color=\"#1d479b\" id=\"section_3\">**3. Création du réseau**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_nc (int) -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            ngf (int) -- the number of filters in the last conv layer\n",
    "        Returns a generator\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Down sampling\n",
    "            nn.Conv2d(input_nc, ngf, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size.  ngf x (image_size/2) x (image_size/2)\n",
    "            \n",
    "            nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ngf*2 x (image_size/4) x (image_size/4)\n",
    "            \n",
    "            nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ngf*4 x (image_size/8) x (image_size/8)\n",
    "            \n",
    "            nn.Conv2d(ngf*4, ngf*8, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ngf*8 x (image_size/16) x (image_size/16)\n",
    "            \n",
    "            # ---\n",
    "            # Up sampling\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ngf*2 x (image_size/4) x (image_size/4)\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x image_size/2 x image_size/2\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, output_nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (output_nc) x image_size x image_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8178f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24a219",
   "metadata": {},
   "source": [
    "## <font color=\"#1d479b\" id=\"section_4\">**4. Modèle CycleGAN**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANModel():\n",
    "    def __init__(self, opt):\n",
    "        # specify the training losses you want to print out.\n",
    "        \n",
    "        self.opt = opt\n",
    "        self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n",
    "        \n",
    "        # specify the images you want to save/display.\n",
    "        visual_names_A = ['real_A', 'fake_B', 'rec_A', 'idt_B']\n",
    "        visual_names_B = ['real_B', 'fake_A', 'rec_B', 'idt_A']\n",
    "        \n",
    "        self.visual_names = visual_names_A + visual_names_B  \n",
    "        \n",
    "        self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\n",
    "        self.save_dir = opt.save_dir\n",
    "        \n",
    "        self.netG_A  = ResnetGenerator(opt.input_nc, opt.output_nc, opt.ngf, n_blocks=4).to(opt.device)\n",
    "        self.netG_B  = ResnetGenerator(opt.input_nc, opt.output_nc, opt.ngf, n_blocks=4).to(opt.device)\n",
    "\n",
    "        self.netD_A = Discriminator(opt.output_nc, opt.ndf).to(opt.device)\n",
    "        self.netD_B = Discriminator(opt.input_nc, opt.ndf).to(opt.device)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.netG_A.apply(weights_init)\n",
    "        self.netG_B.apply(weights_init)\n",
    "        self.netD_A.apply(weights_init)\n",
    "        self.netD_B.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(self):\n",
    "        # forward\n",
    "        self.forward()      # compute fake images and reconstruction images.\n",
    "        # G_A and G_B\n",
    "        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n",
    "        self.optimizer_G.zero_grad() \n",
    "        self.backward_G()            \n",
    "        # D_A and D_B\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def backward_G(self):\n",
    "        lambda_idt = self.opt.lambda_identity\n",
    "        lambda_A = self.opt.lambda_A\n",
    "        lambda_B = self.opt.lambda_B\n",
    "        # Identity loss\n",
    "        if lambda_idt > 0:\n",
    "            self.idt_A = self.netG_A(self.real_B)\n",
    "            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "            self.idt_B = self.netG_B(self.real_A)\n",
    "            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "        else:\n",
    "            self.loss_idt_A = 0\n",
    "            self.loss_idt_B = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
